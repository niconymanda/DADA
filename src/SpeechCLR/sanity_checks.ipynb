{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 17:57:25.280959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732467445.303868 2489434 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732467445.310418 2489434 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 17:57:25.337865: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "from utils.datasets import InTheWildDataset\n",
    "from utils.metrics import ABXAccuracy\n",
    "from utils.training import SpeechCLRTrainerVanilla\n",
    "\n",
    "from models import SpeechEmbedder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import InTheWildDataset\n",
    "\n",
    "\n",
    "train_dataset = InTheWildDataset(\n",
    "        root_dir=\"/home/infres/amathur-23/DADA/datasets/InTheWild\",\n",
    "        metadata_file='meta.csv',\n",
    "        include_spoofs=False,\n",
    "        bonafide_label=\"bona-fide\",\n",
    "        filename_col=\"file\",\n",
    "        sampling_rate=16000,\n",
    "        max_duration=4,\n",
    "        split=\"train\",\n",
    "        config='configs/data/inthewild_toy.yaml',\n",
    "        mode=\"triplet\",\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SpeechEmbedder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaTriplet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Triplet Loss from\n",
    "    Nyugen et al. 'AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning \n",
    "                   for Forensic Medical Image Matching'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, K_d=2, K_an=2, eps=0, beta=0, lambda_=1):\n",
    "        super(AdaTriplet, self).__init__()\n",
    "        self.K_d = K_d  \n",
    "        self.K_an = K_an\n",
    "        self.eps = eps\n",
    "        self.beta = beta\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # stats, init?\n",
    "        self.mu_d = 0\n",
    "        self.mu_an = 0\n",
    "        self.counter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.mu_d = 0\n",
    "        self.mu_an = 0\n",
    "        self.counter = 0\n",
    "\n",
    "    def update_stats(self, phi_ap, phi_an):\n",
    "        delta = phi_ap - phi_an\n",
    "        self.mu_d = (self.counter * self.mu_d + delta.mean()) / (self.counter + 1)\n",
    "        self.mu_an = (self.counter * self.mu_an + phi_an.mean()) / (self.counter + 1)\n",
    "        self.counter = self.counter + 1\n",
    "\n",
    "    def update_margins(self):\n",
    "        self.eps = self.mu_d / self.K_d\n",
    "        self.beta = self.mu_an / self.K_an\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"AdaTriplet(K_d={self.K_d}, K_an={self.K_an}, eps={self.eps}, beta={self.beta}, lambda_={self.lambda_})\"\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        phi_ap = cosine_similarity(anchor, positive)\n",
    "        phi_an = cosine_similarity(anchor, negative)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.update_stats(phi_ap, phi_an)\n",
    "            self.update_margins()\n",
    "\n",
    "        loss = torch.clamp_min(phi_an - phi_ap + self.eps, 0) \n",
    "        loss = loss + self.lambda_ * torch.clamp_min(phi_an - self.beta, 0)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = AdaTriplet(K_d=2, K_an=2, eps=0, beta=0, lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaTriplet(K_d=2, K_an=2, eps=0, beta=0, lambda_=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "n = 0\n",
    "for _ in tqdm(range(len(train_loader))[:5]):\n",
    "    batch = next(iter(train_loader))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    output = model(batch)\n",
    "    loss = criterion(output[\"anchor\"], output[\"positive\"], output[\"negative\"])\n",
    "    try:\n",
    "        # assert not torch.isnan(loss), \"loss is nan\"\n",
    "        loss.backward()\n",
    "    except Exception as e:\n",
    "        n+=1\n",
    "        print(e)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaTriplet(K_d=2, K_an=2, eps=0.04331747442483902, beta=0.360461950302124, lambda_=1)\n"
     ]
    }
   ],
   "source": [
    "print(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
