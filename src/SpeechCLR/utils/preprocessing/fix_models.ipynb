{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "2024-11-17 23:34:11.399130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731882851.420468  319509 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731882851.426954  319509 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 23:34:11.449985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from speechbrain.lobes.models.ECAPA_TDNN import AttentiveStatisticsPooling\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "BASE_MODEL = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "PROC_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "# LINGUISTICS_MODEL = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import InTheWildDataset\n",
    "\n",
    "\n",
    "train_dataset = InTheWildDataset(\n",
    "        root_dir=\"/home/infres/amathur-23/DADA/datasets/InTheWild\",\n",
    "        metadata_file='meta.csv',\n",
    "        include_spoofs=False,\n",
    "        bonafide_label=\"bona-fide\",\n",
    "        filename_col=\"file\",\n",
    "        sampling_rate=16000,\n",
    "        max_duration=4,\n",
    "        split=\"train\",\n",
    "        config='configs/data/inthewild_toy.yaml',\n",
    "        mode=\"triplet\",\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = next(iter(train_loader))\n",
    "input = {k: v.to(device) for k, v in input.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, K = 199, F_in=1024, F_out=256, bottleneck_dropout=0.5):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.bottleneck_dropout = bottleneck_dropout\n",
    "\n",
    "\n",
    "        self.lin1 =  nn.Linear(F_in, F_out)\n",
    "        self.bn1  =  nn.BatchNorm1d(K)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.drop =  nn.Dropout(self.bottleneck_dropout)\n",
    "        self.lin2 = nn.Linear(F_out, F_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "class CompressionModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, K=199, F_in=1024, F_out=256, bottleneck_dropout=0.5, head_dropout=0.5\n",
    "    ):\n",
    "        super(CompressionModule, self).__init__()\n",
    "        self.K = K\n",
    "        self.bottleneck_dropout = bottleneck_dropout\n",
    "        self.head_dropout = head_dropout\n",
    "        self.pool = lambda x : torch.mean(x, dim=-1)\n",
    "\n",
    "        self.bottleneck = BottleNeck(K, F_in, F_out, bottleneck_dropout)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(self.head_dropout),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(F_in, F_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pool = self.pool(x)\n",
    "        x = self.bottleneck(x_pool)\n",
    "        x = self.head(x + x_pool)\n",
    "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_layers=(0, 10),\n",
    "        K=199,\n",
    "        F_in=1024,\n",
    "        F_out=256,\n",
    "        bottleneck_dropout=0.5,\n",
    "        head_dropout=0.5,\n",
    "        device = 'cuda'\n",
    "    ):\n",
    "        super(SpeechEmbedder, self).__init__()\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(PROC_ID)\n",
    "        self.feature_model = Wav2Vec2ForCTC.from_pretrained(BASE_MODEL)\n",
    "        self.feature_model.eval()\n",
    "        self.compression = CompressionModule(\n",
    "            K=K,\n",
    "            F_in=F_in,\n",
    "            F_out=F_out,\n",
    "            bottleneck_dropout=bottleneck_dropout,\n",
    "            head_dropout=head_dropout,\n",
    "        )\n",
    "        self.feature_layers = feature_layers\n",
    "        self.device = device\n",
    "\n",
    "    def get_features(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.processor(x, return_tensors=\"pt\", padding=True, sampling_rate=16_000, device=self.device).input_values[0]\n",
    "            x = x.to(self.device)\n",
    "            out = self.feature_model(x, output_hidden_states=True, return_dict=True)\n",
    "            feat = torch.stack(out.hidden_states[self.feature_layers[0] : self.feature_layers[1]], dim =-1)\n",
    "        return feat\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.compression.to(device)\n",
    "        self.feature_model.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    def train(self):\n",
    "        self.compression.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.compression.eval()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        a, p, n = input[\"anchor\"], input[\"positive\"], input[\"negative\"]\n",
    "        x_a, x_p, x_n = self.get_features(a), self.get_features(p), self.get_features(n)\n",
    "\n",
    "        x_a, x_p, x_n = (\n",
    "            self.compression(x_a),\n",
    "            self.compression(x_p),\n",
    "            self.compression(x_n),\n",
    "        )\n",
    "\n",
    "        x_a, x_p, x_n = (\n",
    "            rearrange(x_a, \"n t f -> n f t\"),\n",
    "            rearrange(x_p, \"n t f -> n f t\"),\n",
    "            rearrange(x_n, \"n t f -> n f t\"),\n",
    "        )\n",
    "\n",
    "        x_a, x_p, x_n = (\n",
    "            x_a.mean(dim=-1),\n",
    "            x_p.mean(dim=-1),\n",
    "            x_n.mean(dim=-1),\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"anchor\": x_a,\n",
    "            \"positive\": x_p,\n",
    "            \"negative\": x_n,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SpeechEmbedder().to(device)\n",
    "\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 256]), torch.Size([4, 256]), torch.Size([4, 256])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in output.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
