{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/infres/amathur-23/DADA/src/mid_fusion\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/infres/amathur-23/DADA/src\n"
     ]
    }
   ],
   "source": [
    "%cd /home/infres/amathur-23/DADA/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['HF_HOME'] = '/data/amathur-23/DADA/transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from authorship_attribution.model import AuthorshipLLM\n",
    "from SpeechCLR.models import SpeechEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502b2c5d2e4b4939a27817d7ad1a63c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea19d8b290c4730ad0447e31c922f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEXT_MODEL_NAME = 'microsoft/deberta-v3-large'\n",
    "\n",
    "text_model = AuthorshipLLM(\n",
    "    model_name=TEXT_MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "speech_model = SpeechEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuthorshipLLM(\n",
       "  (model): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechEmbedder(\n",
       "  (feature_model): Wav2Vec2ForCTC(\n",
       "    (wav2vec2): Wav2Vec2Model(\n",
       "      (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): Wav2Vec2FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "        (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (padding): Wav2Vec2SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "            (attention): Wav2Vec2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): Wav2Vec2FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.05, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (output_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (lm_head): Linear(in_features=1024, out_features=33, bias=True)\n",
       "  )\n",
       "  (compression): CompressionModule(\n",
       "    (bottleneck): BottleNeck(\n",
       "      (lin1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (bn1): BatchNorm1d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (lin2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../datasets/InTheWild/wild_transcription_meta.json\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>content</th>\n",
       "      <th>speaker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11416.wav</td>\n",
       "      <td>In fact, dead people.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>bona-fide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11424.wav</td>\n",
       "      <td>and the local government's act for whiz.</td>\n",
       "      <td>FDR</td>\n",
       "      <td>bona-fide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11422.wav</td>\n",
       "      <td>But I have made clear our trade imbalance is ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>bona-fide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11426.wav</td>\n",
       "      <td>under very difficult circumstances is far gre...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>bona-fide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11420.wav</td>\n",
       "      <td>in the modern history of the United States of...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>bona-fide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                            content  \\\n",
       "0  11416.wav                              In fact, dead people.   \n",
       "1  11424.wav           and the local government's act for whiz.   \n",
       "2  11422.wav   But I have made clear our trade imbalance is ...   \n",
       "3  11426.wav   under very difficult circumstances is far gre...   \n",
       "4  11420.wav   in the modern history of the United States of...   \n",
       "\n",
       "          speaker      label  \n",
       "0    Donald Trump  bona-fide  \n",
       "1             FDR  bona-fide  \n",
       "2    Donald Trump  bona-fide  \n",
       "3    Barack Obama  bona-fide  \n",
       "4  Bernie Sanders  bona-fide  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import yaml\n",
    "\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "\n",
    "SUPPORTED_FORMATS = [\"wav\", \"mp3\", \"flac\"]\n",
    "\n",
    "from mid_fusion.utils.datasets import load_audio, pad, get_spoof_list\n",
    "\n",
    "class InTheWildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        metadata_file=\"meta.csv\",\n",
    "        include_spoofs=False,\n",
    "        bonafide_label=\"bona-fide\",\n",
    "        filename_col=\"file\",\n",
    "        transcription_col='content',\n",
    "        sampling_rate=16000,\n",
    "        max_duration=4,\n",
    "        split=\"train\",\n",
    "        config=None,\n",
    "        max_text_length = 64,\n",
    "        mode=\"classification\",\n",
    "        text_tokenizer_name = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the dataset\n",
    "            metadata_file (str): Name of the metadata file\n",
    "            include_spoofs (bool): Whether to include spoofed data\n",
    "            bonafide_label (str): Label for bonafide data\n",
    "            sampling_rate (int): Sampling rate of the audio files\n",
    "            max_duration (int): Maximum duration of the audio files in seconds\n",
    "            split (str): Split of the dataset (train, val, test)\n",
    "            config (dict): Configuration dictionary\n",
    "            mode (str): Mode of the dataset (triplet, classification)\n",
    "        \"\"\"\n",
    "\n",
    "        if text_tokenizer_name is None:\n",
    "            raise NotImplementedError(\"Text tokenizer is required for this dataset\")\n",
    "        \n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_name)\n",
    "        self.max_text_length = max_text_length\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata_file = metadata_file\n",
    "        if self.metadata_file.endswith(\".csv\"):\n",
    "            self.df = pd.read_csv(os.path.join(root_dir, metadata_file))\n",
    "        elif self.metadata_file.endswith(\".json\"):\n",
    "            self.df = pd.read_json(os.path.join(root_dir, metadata_file))\n",
    "        self.include_spoofs = include_spoofs\n",
    "        self.split = split\n",
    "        self.mode = mode\n",
    "\n",
    "        self.bonafide_label = bonafide_label\n",
    "\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_duration = max_duration\n",
    "        self.cut = self.sampling_rate * self.max_duration  # padding\n",
    "\n",
    "        # Filter out spoofed data if include_spoofs is False\n",
    "        if not self.include_spoofs:\n",
    "            self.df = self.df[self.df[\"label\"] == bonafide_label]\n",
    "\n",
    "        # Filter out data based on filename in config['split']\n",
    "        if config is not None:\n",
    "            self.config = yaml.safe_load(open(config, \"r\"))\n",
    "            self.df = self.df[self.df[filename_col].isin(self.config[split])]\n",
    "\n",
    "        # Create id to filename mapping and id to label mapping\n",
    "        ids = np.arange(len(self.df))\n",
    "        filenames = self.df[filename_col].values\n",
    "        labels = self.df[\"label\"].values\n",
    "        authors = self.df[\"speaker\"].values\n",
    "        transcriptions = self.df[transcription_col].values\n",
    "\n",
    "        self.id_to_filename = dict(zip(ids, filenames))\n",
    "        self.id_to_label = dict(zip(ids, labels))\n",
    "        self.id_to_author = dict(zip(ids, authors))\n",
    "        self.id_to_transcription = dict(zip(ids, transcriptions))\n",
    "\n",
    "    def text_to_input_dict(self, text):\n",
    "        inputs = self.text_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_text_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        inputs = {k: v.squeeze() for k, v in inputs.items()}\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def load_audio_tensor(self, idx):\n",
    "        # Load audio file\n",
    "        # print(idx)\n",
    "        try:\n",
    "            filename = os.path.join(self.root_dir, self.id_to_filename[idx])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(idx)\n",
    "        audio_arr, _ = load_audio(filename, self.sampling_rate)\n",
    "        audio_tensor = torch.tensor(pad(audio_arr, self.cut)).float()\n",
    "        return audio_tensor\n",
    "\n",
    "    def get_triplets_from_anchor(self, anchor_idx):\n",
    "        anchor_author = self.id_to_author[anchor_idx]\n",
    "        positive_id = np.random.choice(\n",
    "            np.where(self.df[\"speaker\"].values == anchor_author)[0]\n",
    "        )\n",
    "        negative_ids = np.where(self.df[\"speaker\"].values != anchor_author)[0]\n",
    "\n",
    "        return positive_id, np.random.choice(negative_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_to_filename)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.load_audio_tensor(idx)\n",
    "        y = int(self.id_to_label[idx] == self.bonafide_label)\n",
    "        author = self.id_to_author[idx]\n",
    "        transcription = self.text_to_input_dict(self.id_to_transcription[idx])\n",
    "\n",
    "        if self.mode == \"classification\":\n",
    "            return {\"x\": x, \"label\": y, \"author\": author, \"transcription\": transcription}\n",
    "\n",
    "        elif self.mode == \"triplet\":\n",
    "            id_p, id_n = self.get_triplets_from_anchor(idx)\n",
    "\n",
    "            x_p = self.load_audio_tensor(id_p)\n",
    "            x_n = self.load_audio_tensor(id_n)\n",
    "\n",
    "            return {\"anchor\": x, \"positive\": x_p, \"negative\": x_n}\n",
    "        \n",
    "        elif self.mode == \"pair\":\n",
    "            a = x\n",
    "            a_label = author\n",
    "\n",
    "            # Choose another idx at random\n",
    "            idx2 = np.random.choice(np.arange(len(self.df)))\n",
    "            b = self.load_audio_tensor(idx2)\n",
    "            b_label = self.id_to_author[idx2]\n",
    "\n",
    "            return {\"a\": a, \"b\": b, \"a_label\": a_label, \"b_label\": b_label}\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode {self.mode} not implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TEXT_FEATURES = 1024\n",
    "SPEECH_FEATURES = 256\n",
    "\n",
    "dataset = InTheWildDataset(\n",
    "    root_dir=\"../datasets/InTheWild\",\n",
    "    metadata_file=\"wild_transcription_meta.json\",\n",
    "    include_spoofs=True,\n",
    "    bonafide_label=\"bona-fide\",\n",
    "    filename_col=\"file\",\n",
    "    transcription_col='content',\n",
    "    sampling_rate=16000,\n",
    "    max_duration=4,\n",
    "    split=\"train\",\n",
    "    text_tokenizer_name=TEXT_MODEL_NAME,\n",
    "    config=\"/home/infres/amathur-23/DADA/src/SpeechCLR/configs/data/inthewild_toy.yaml\", \n",
    "    mode=\"classification\",\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[-1.7785e-04, -3.3424e-04, -9.4488e-05,  ..., -3.9154e-02,\n",
       "          -3.5004e-02, -2.8067e-02],\n",
       "         [-4.8968e-04, -1.8645e-04, -1.6061e-03,  ..., -1.9415e-02,\n",
       "           7.7594e-03,  2.0707e-02],\n",
       "         [ 1.5939e-04,  1.6132e-03,  3.0111e-03,  ..., -2.5401e-03,\n",
       "          -2.3908e-03, -2.1925e-03],\n",
       "         [-4.9476e-07,  4.5353e-05,  7.9203e-05,  ...,  6.8018e-02,\n",
       "           6.9764e-02,  6.9907e-02]]),\n",
       " 'label': tensor([1, 0, 0, 0]),\n",
       " 'author': ['Bernie Sanders',\n",
       "  'Alec Guinness',\n",
       "  'Donald Trump',\n",
       "  'Bernie Sanders'],\n",
       " 'transcription': {'input_ids': tensor([[    1,   379,   261,   379,   459,  8254,   260,     2,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [    1,   304,   319,   313,  5629,   264, 10731,   293,   262,   823,\n",
       "            1655, 52099,  8031,  1950, 66043,   265,  3527,   260,     2,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [    1, 25812,   290,  1959,   260,     2,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [    1,   344,   308,  1365,   262,  2326,  1823,   296,   260,     2,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]]),\n",
       "  'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidFuse(nn.Module):\n",
    "    def __init__(self, text_model, speech_model, text_features, speech_features):\n",
    "        super(MidFuse, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.speech_model = speech_model\n",
    "        self.text_features = text_features\n",
    "        self.speech_features = speech_features\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(text_features + speech_features, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        self.text_model.eval()\n",
    "        self.speech_model.eval()\n",
    "        self.classifier.train()\n",
    "\n",
    "    def trainable_parameters(self):\n",
    "        return self.classifier.parameters()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.text_model.eval()\n",
    "        self.speech_model.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "    def forward(self, text_input, speech_input):\n",
    "        text_features = self.text_model(input_ids = text_input['input_ids'], attention_mask = text_input['attention_mask'])\n",
    "        speech_features = self.speech_model(speech_input, mode='classification')\n",
    "        features = torch.cat([text_features, speech_features], dim=1)\n",
    "        x = self.classifier(features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_fuse_model = MidFuse(text_model, speech_model, TEXT_FEATURES, SPEECH_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MidFuse(\n",
       "  (text_model): AuthorshipLLM(\n",
       "    (model): DebertaV2Model(\n",
       "      (embeddings): DebertaV2Embeddings(\n",
       "        (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): StableDropout()\n",
       "      )\n",
       "      (encoder): DebertaV2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (pos_dropout): StableDropout()\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): StableDropout()\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rel_embeddings): Embedding(512, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (speech_model): SpeechEmbedder(\n",
       "    (feature_model): Wav2Vec2ForCTC(\n",
       "      (wav2vec2): Wav2Vec2Model(\n",
       "        (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Wav2Vec2LayerNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): Wav2Vec2FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "          (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "            (conv): ParametrizedConv1d(\n",
       "              1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (padding): Wav2Vec2SamePadLayer()\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "              (attention): Wav2Vec2SdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "              (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (feed_forward): Wav2Vec2FeedForward(\n",
       "                (intermediate_dropout): Dropout(p=0.05, inplace=False)\n",
       "                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (output_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (lm_head): Linear(in_features=1024, out_features=33, bias=True)\n",
       "    )\n",
       "    (compression): CompressionModule(\n",
       "      (bottleneck): BottleNeck(\n",
       "        (lin1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01)\n",
       "        (drop): Dropout(p=0.5, inplace=False)\n",
       "        (lin2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_fuse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 [285/285] Loss: 0.50141384035610316\n",
      " Epoch 2 [120/285] Loss: 0.46988484381387636"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(mid_fuse_model.trainable_parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "mid_fuse_model.train()\n",
    "mid_fuse_model.to('cuda')\n",
    "\n",
    "for epoch in range(5):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        text = batch['transcription']\n",
    "        speech = batch['x']\n",
    "        label = batch['label'].squeeze().float().to('cuda')\n",
    "\n",
    "        text = {k: v.to('cuda') for k, v in text.items()}\n",
    "        batch['x'] = batch['x'].to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mid_fuse_model(text_input = text, speech_input = batch).squeeze()\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # if i % 100 == 0:\n",
    "        print(f\"\\r Epoch {epoch + 1} [{i+1}/{len(loader)}] Loss: {np.mean(losses)}\", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
