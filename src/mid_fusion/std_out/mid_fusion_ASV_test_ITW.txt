nohup: ignoring input
2025-02-07 17:54:27.035329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738947267.057110 2038145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738947267.063665 2038145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-07 17:54:27.087062: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/infres/amathur-23/DADA/dada/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/home/infres/amathur-23/DADA/src/SpeechCLR/models.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(path)
Using GPU: 1
Loaded Dataset - 
Training Samples : 1868
Validation Samples : 527
Logging to mid_fusion/runs/mid_fusion_ASV_test_ITW
Using device: cuda
Loaded text model from /data/amathur-23/DADA/models/text/Deberta/final
Loaded speech model from /data/amathur-23/DADA/models/SpeechEmbedder/tricos_p35_10v8/best_model.pth
Loaded mid-fusion model from /data/amathur-23/DADA/models/mid_fusion/mid_fusion_trial_on_ASV/runs/best_model.pth
Getting Inital Metrics
Evaluation on train [1/234] loss: 1.117, acc: 0.625   Evaluation on train [2/234] loss: 1.056, acc: 0.750   Evaluation on train [3/234] loss: 0.878, acc: 0.792   Evaluation on train [4/234] loss: 0.734, acc: 0.812   Evaluation on train [5/234] loss: 0.704, acc: 0.825   Evaluation on train [6/234] loss: 1.002, acc: 0.771   Evaluation on train [7/234] loss: 1.031, acc: 0.750   Evaluation on train [8/234] loss: 0.947, acc: 0.766   Evaluation on train [9/234] loss: 0.867, acc: 0.778   Evaluation on train [10/234] loss: 0.807, acc: 0.775   Evaluation on train [11/234] loss: 0.773, acc: 0.761   Evaluation on train [12/234] loss: 0.863, acc: 0.750   Evaluation on train [13/234] loss: 0.844, acc: 0.740   Evaluation on train [14/234] loss: 0.849, acc: 0.732   Evaluation on train [15/234] loss: 0.869, acc: 0.725   Evaluation on train [16/234] loss: 0.877, acc: 0.719   Evaluation on train [17/234] loss: 0.866, acc: 0.721   Evaluation on train [18/234] loss: 0.847, acc: 0.722   Evaluation on train [19/234] loss: 0.898, acc: 0.711   Evaluation on train [20/234] loss: 0.889, acc: 0.706   Evaluation on train [21/234] loss: 0.876, acc: 0.708   Evaluation on train [22/234] loss: 0.896, acc: 0.705   Evaluation on train [23/234] loss: 0.911, acc: 0.696   Evaluation on train [24/234] loss: 0.906, acc: 0.698   Evaluation on train [25/234] loss: 0.873, acc: 0.710   Evaluation on train [26/234] loss: 0.924, acc: 0.697   Evaluation on train [27/234] loss: 0.896, acc: 0.704   Evaluation on train [28/234] loss: 0.877, acc: 0.705   Evaluation on train [29/234] loss: 0.855, acc: 0.711   Evaluation on train [30/234] loss: 0.872, acc: 0.713   Evaluation on train [31/234] loss: 0.884, acc: 0.714   Evaluation on train [32/234] loss: 0.890, acc: 0.707   Evaluation on train [33/234] loss: 0.865, acc: 0.716   Evaluation on train [34/234] loss: 0.851, acc: 0.721   Evaluation on train [35/234] loss: 0.840, acc: 0.721   Evaluation on train [36/234] loss: 0.855, acc: 0.719   Evaluation on train [37/234] loss: 0.863, acc: 0.716   Evaluation on train [38/234] loss: 0.897, acc: 0.707   Evaluation on train [39/234] loss: 0.894, acc: 0.708   Evaluation on train [40/234] loss: 0.887, acc: 0.713   Evaluation on train [41/234] loss: 0.886, acc: 0.713   Evaluation on train [42/234] loss: 0.881, acc: 0.714   Evaluation on train [43/234] loss: 0.862, acc: 0.721   Evaluation on train [44/234] loss: 0.856, acc: 0.722   Evaluation on train [45/234] loss: 0.858, acc: 0.714   Evaluation on train [46/234] loss: 0.867, acc: 0.712   Evaluation on train [47/234] loss: 0.869, acc: 0.710   Evaluation on train [48/234] loss: 0.862, acc: 0.711   Evaluation on train [49/234] loss: 0.847, acc: 0.714   Evaluation on train [50/234] loss: 0.838, acc: 0.718   Evaluation on train [51/234] loss: 0.828, acc: 0.718   Evaluation on train [52/234] loss: 0.814, acc: 0.724   Evaluation on train [53/234] loss: 0.812, acc: 0.722   Evaluation on train [54/234] loss: 0.802, acc: 0.725   Evaluation on train [55/234] loss: 0.809, acc: 0.725   Evaluation on train [56/234] loss: 0.810, acc: 0.725   Evaluation on train [57/234] loss: 0.812, acc: 0.724   Evaluation on train [58/234] loss: 0.798, acc: 0.728   Evaluation on train [59/234] loss: 0.803, acc: 0.727   Evaluation on train [60/234] loss: 0.796, acc: 0.729   Evaluation on train [61/234] loss: 0.787, acc: 0.732   Evaluation on train [62/234] loss: 0.796, acc: 0.732   Evaluation on train [63/234] loss: 0.789, acc: 0.734   Evaluation on train [64/234] loss: 0.787, acc: 0.734   Evaluation on train [65/234] loss: 0.808, acc: 0.733   Evaluation on train [66/234] loss: 0.801, acc: 0.733   Evaluation on train [67/234] loss: 0.789, acc: 0.737   Evaluation on train [68/234] loss: 0.780, acc: 0.741   Evaluation on train [69/234] loss: 0.776, acc: 0.741   Evaluation on train [70/234] loss: 0.774, acc: 0.741   Evaluation on train [71/234] loss: 0.773, acc: 0.739   Evaluation on train [72/234] loss: 0.763, acc: 0.743   Evaluation on train [73/234] loss: 0.762, acc: 0.741   Evaluation on train [74/234] loss: 0.758, acc: 0.742   Evaluation on train [75/234] loss: 0.753, acc: 0.742   Evaluation on train [76/234] loss: 0.760, acc: 0.737   Evaluation on train [77/234] loss: 0.767, acc: 0.734   Evaluation on train [78/234] loss: 0.761, acc: 0.736   Evaluation on train [79/234] loss: 0.760, acc: 0.736   Evaluation on train [80/234] loss: 0.760, acc: 0.736   Evaluation on train [81/234] loss: 0.759, acc: 0.735   Evaluation on train [82/234] loss: 0.753, acc: 0.736   Evaluation on train [83/234] loss: 0.751, acc: 0.736   Evaluation on train [84/234] loss: 0.749, acc: 0.737   Evaluation on train [85/234] loss: 0.742, acc: 0.740   Evaluation on train [86/234] loss: 0.741, acc: 0.740   Evaluation on train [87/234] loss: 0.737, acc: 0.741   Evaluation on train [88/234] loss: 0.743, acc: 0.739   Evaluation on train [89/234] loss: 0.740, acc: 0.740   Evaluation on train [90/234] loss: 0.735, acc: 0.739   Evaluation on train [91/234] loss: 0.740, acc: 0.736   Evaluation on train [92/234] loss: 0.743, acc: 0.735   Evaluation on train [93/234] loss: 0.754, acc: 0.733   Evaluation on train [94/234] loss: 0.760, acc: 0.733   Evaluation on train [95/234] loss: 0.754, acc: 0.734   Evaluation on train [96/234] loss: 0.749, acc: 0.734   Evaluation on train [97/234] loss: 0.759, acc: 0.732   Evaluation on train [98/234] loss: 0.761, acc: 0.731   Evaluation on train [99/234] loss: 0.760, acc: 0.731   Evaluation on train [100/234] loss: 0.769, acc: 0.730   Evaluation on train [101/234] loss: 0.772, acc: 0.728   Evaluation on train [102/234] loss: 0.767, acc: 0.729   Evaluation on train [103/234] loss: 0.767, acc: 0.728   Evaluation on train [104/234] loss: 0.763, acc: 0.728   Evaluation on train [105/234] loss: 0.765, acc: 0.727   Evaluation on train [106/234] loss: 0.769, acc: 0.729   Evaluation on train [107/234] loss: 0.773, acc: 0.727   Evaluation on train [108/234] loss: 0.772, acc: 0.726   Evaluation on train [109/234] loss: 0.765, acc: 0.728   Evaluation on train [110/234] loss: 0.758, acc: 0.731   Evaluation on train [111/234] loss: 0.755, acc: 0.732   Evaluation on train [112/234] loss: 0.757, acc: 0.731   Evaluation on train [113/234] loss: 0.753, acc: 0.732   Evaluation on train [114/234] loss: 0.751, acc: 0.734   Evaluation on train [115/234] loss: 0.756, acc: 0.733   Evaluation on train [116/234] loss: 0.754, acc: 0.734   Evaluation on train [117/234] loss: 0.764, acc: 0.731   Evaluation on train [118/234] loss: 0.766, acc: 0.730   Evaluation on train [119/234] loss: 0.765, acc: 0.730   Evaluation on train [120/234] loss: 0.770, acc: 0.729   Evaluation on train [121/234] loss: 0.768, acc: 0.728   Evaluation on train [122/234] loss: 0.764, acc: 0.728   Evaluation on train [123/234] loss: 0.768, acc: 0.727   Evaluation on train [124/234] loss: 0.766, acc: 0.727   Evaluation on train [125/234] loss: 0.763, acc: 0.727   Evaluation on train [126/234] loss: 0.765, acc: 0.726   Evaluation on train [127/234] loss: 0.771, acc: 0.723   Evaluation on train [128/234] loss: 0.765, acc: 0.725   Evaluation on train [129/234] loss: 0.768, acc: 0.724   Evaluation on train [130/234] loss: 0.771, acc: 0.722   Evaluation on train [131/234] loss: 0.767, acc: 0.723   Evaluation on train [132/234] loss: 0.775, acc: 0.721   Evaluation on train [133/234] loss: 0.776, acc: 0.720   Evaluation on train [134/234] loss: 0.780, acc: 0.718   Evaluation on train [135/234] loss: 0.782, acc: 0.719   Evaluation on train [136/234] loss: 0.781, acc: 0.718   Evaluation on train [137/234] loss: 0.779, acc: 0.719   Evaluation on train [138/234] loss: 0.783, acc: 0.717   Evaluation on train [139/234] loss: 0.783, acc: 0.717   Evaluation on train [140/234] loss: 0.782, acc: 0.716   Evaluation on train [141/234] loss: 0.782, acc: 0.716   Evaluation on train [142/234] loss: 0.790, acc: 0.715   Evaluation on train [143/234] loss: 0.790, acc: 0.712   Evaluation on train [144/234] loss: 0.785, acc: 0.714   Evaluation on train [145/234] loss: 0.784, acc: 0.715   Evaluation on train [146/234] loss: 0.790, acc: 0.714   Evaluation on train [147/234] loss: 0.788, acc: 0.714   Evaluation on train [148/234] loss: 0.786, acc: 0.715   Evaluation on train [149/234] loss: 0.783, acc: 0.716   Evaluation on train [150/234] loss: 0.782, acc: 0.715   Evaluation on train [151/234] loss: 0.784, acc: 0.714   Evaluation on train [152/234] loss: 0.781, acc: 0.715   Evaluation on train [153/234] loss: 0.780, acc: 0.714   Evaluation on train [154/234] loss: 0.778, acc: 0.714   Evaluation on train [155/234] loss: 0.774, acc: 0.715   Evaluation on train [156/234] loss: 0.773, acc: 0.716   Evaluation on train [157/234] loss: 0.771, acc: 0.716   Evaluation on train [158/234] loss: 0.770, acc: 0.715   Evaluation on train [159/234] loss: 0.778, acc: 0.711   Evaluation on train [160/234] loss: 0.779, acc: 0.711   Evaluation on train [161/234] loss: 0.776, acc: 0.712   Evaluation on train [162/234] loss: 0.775, acc: 0.711   Evaluation on train [163/234] loss: 0.778, acc: 0.710   Evaluation on train [164/234] loss: 0.778, acc: 0.710   Evaluation on train [165/234] loss: 0.780, acc: 0.711   Evaluation on train [166/234] loss: 0.782, acc: 0.711   Evaluation on train [167/234] loss: 0.786, acc: 0.710   Evaluation on train [168/234] loss: 0.783, acc: 0.711   Evaluation on train [169/234] loss: 0.784, acc: 0.711   Evaluation on train [170/234] loss: 0.784, acc: 0.711   Evaluation on train [171/234] loss: 0.785, acc: 0.711   Evaluation on train [172/234] loss: 0.785, acc: 0.711   Evaluation on train [173/234] loss: 0.784, acc: 0.710   Evaluation on train [174/234] loss: 0.783, acc: 0.710   Evaluation on train [175/234] loss: 0.787, acc: 0.709   Evaluation on train [176/234] loss: 0.791, acc: 0.708   Evaluation on train [177/234] loss: 0.793, acc: 0.706   Evaluation on train [178/234] loss: 0.795, acc: 0.706   Evaluation on train [179/234] loss: 0.794, acc: 0.707   Evaluation on train [180/234] loss: 0.796, acc: 0.706   Evaluation on train [181/234] loss: 0.796, acc: 0.705   Evaluation on train [182/234] loss: 0.797, acc: 0.706   Evaluation on train [183/234] loss: 0.794, acc: 0.707   Evaluation on train [184/234] loss: 0.790, acc: 0.709   Evaluation on train [185/234] loss: 0.787, acc: 0.709   Evaluation on train [186/234] loss: 0.784, acc: 0.710   Evaluation on train [187/234] loss: 0.782, acc: 0.711   Evaluation on train [188/234] loss: 0.784, acc: 0.711   Evaluation on train [189/234] loss: 0.781, acc: 0.712   Evaluation on train [190/234] loss: 0.782, acc: 0.711   Evaluation on train [191/234] loss: 0.780, acc: 0.711   Evaluation on train [192/234] loss: 0.777, acc: 0.712   Evaluation on train [193/234] loss: 0.779, acc: 0.712   Evaluation on train [194/234] loss: 0.782, acc: 0.711   Evaluation on train [195/234] loss: 0.780, acc: 0.712   Evaluation on train [196/234] loss: 0.780, acc: 0.712   Evaluation on train [197/234] loss: 0.777, acc: 0.713   Evaluation on train [198/234] loss: 0.782, acc: 0.713   Evaluation on train [199/234] loss: 0.787, acc: 0.710   Evaluation on train [200/234] loss: 0.784, acc: 0.711   Evaluation on train [201/234] loss: 0.788, acc: 0.711   Evaluation on train [202/234] loss: 0.789, acc: 0.712   Evaluation on train [203/234] loss: 0.785, acc: 0.712   Evaluation on train [204/234] loss: 0.786, acc: 0.713   Evaluation on train [205/234] loss: 0.784, acc: 0.713   Evaluation on train [206/234] loss: 0.782, acc: 0.714   Evaluation on train [207/234] loss: 0.780, acc: 0.714   Evaluation on train [208/234] loss: 0.779, acc: 0.714   Evaluation on train [209/234] loss: 0.778, acc: 0.715   Evaluation on train [210/234] loss: 0.775, acc: 0.715   Evaluation on train [211/234] loss: 0.773, acc: 0.716   Evaluation on train [212/234] loss: 0.769, acc: 0.718   Evaluation on train [213/234] loss: 0.770, acc: 0.716   Evaluation on train [214/234] loss: 0.774, acc: 0.716   Evaluation on train [215/234] loss: 0.777, acc: 0.716   Evaluation on train [216/234] loss: 0.777, acc: 0.716   Evaluation on train [217/234] loss: 0.777, acc: 0.715   Evaluation on train [218/234] loss: 0.779, acc: 0.713   Evaluation on train [219/234] loss: 0.777, acc: 0.714   Evaluation on train [220/234] loss: 0.778, acc: 0.714   Evaluation on train [221/234] loss: 0.780, acc: 0.713   Evaluation on train [222/234] loss: 0.783, acc: 0.713   Evaluation on train [223/234] loss: 0.787, acc: 0.713   Evaluation on train [224/234] loss: 0.786, acc: 0.713   Evaluation on train [225/234] loss: 0.789, acc: 0.713   Evaluation on train [226/234] loss: 0.791, acc: 0.712   Evaluation on train [227/234] loss: 0.794, acc: 0.712   Evaluation on train [228/234] loss: 0.795, acc: 0.712   Evaluation on train [229/234] loss: 0.795, acc: 0.712   Evaluation on train [230/234] loss: 0.792, acc: 0.713   Evaluation on train [231/234] loss: 0.791, acc: 0.712   Evaluation on train [232/234] loss: 0.791, acc: 0.712   Evaluation on train [233/234] loss: 0.793, acc: 0.711   Evaluation on train [234/234] loss: 0.792, acc: 0.712   Evaluation on train [234/234] loss: 0.792, acc: 0.712, eer: 0.064, f1_score: 0.702   
Evaluation on val [1/66] loss: 1.147, acc: 0.625   Evaluation on val [2/66] loss: 0.861, acc: 0.688   Evaluation on val [3/66] loss: 0.989, acc: 0.667   Evaluation on val [4/66] loss: 0.786, acc: 0.719   Evaluation on val [5/66] loss: 0.826, acc: 0.725   Evaluation on val [6/66] loss: 0.899, acc: 0.708   Evaluation on val [7/66] loss: 0.861, acc: 0.714   Evaluation on val [8/66] loss: 0.867, acc: 0.703   Evaluation on val [9/66] loss: 0.858, acc: 0.681   Evaluation on val [10/66] loss: 0.817, acc: 0.700   Evaluation on val [11/66] loss: 0.767, acc: 0.716   Evaluation on val [12/66] loss: 0.744, acc: 0.729   Evaluation on val [13/66] loss: 0.729, acc: 0.740   Evaluation on val [14/66] loss: 0.863, acc: 0.723   Evaluation on val [15/66] loss: 0.868, acc: 0.717   Evaluation on val [16/66] loss: 0.937, acc: 0.703   Evaluation on val [17/66] loss: 0.893, acc: 0.713   Evaluation on val [18/66] loss: 0.866, acc: 0.722   Evaluation on val [19/66] loss: 0.852, acc: 0.724   Evaluation on val [20/66] loss: 0.879, acc: 0.719   Evaluation on val [21/66] loss: 0.884, acc: 0.720   Evaluation on val [22/66] loss: 0.868, acc: 0.727   Evaluation on val [23/66] loss: 0.835, acc: 0.734   Evaluation on val [24/66] loss: 0.855, acc: 0.729   Evaluation on val [25/66] loss: 0.853, acc: 0.715   Evaluation on val [26/66] loss: 0.860, acc: 0.716   Evaluation on val [27/66] loss: 0.869, acc: 0.708   Evaluation on val [28/66] loss: 0.877, acc: 0.710   Evaluation on val [29/66] loss: 0.914, acc: 0.703   Evaluation on val [30/66] loss: 0.917, acc: 0.700   Evaluation on val [31/66] loss: 0.909, acc: 0.702   Evaluation on val [32/66] loss: 0.903, acc: 0.703   Evaluation on val [33/66] loss: 0.896, acc: 0.701   Evaluation on val [34/66] loss: 0.922, acc: 0.699   Evaluation on val [35/66] loss: 0.929, acc: 0.700   Evaluation on val [36/66] loss: 0.922, acc: 0.698   Evaluation on val [37/66] loss: 0.931, acc: 0.696   Evaluation on val [38/66] loss: 0.942, acc: 0.691   Evaluation on val [39/66] loss: 0.958, acc: 0.689   Evaluation on val [40/66] loss: 0.943, acc: 0.691   Evaluation on val [41/66] loss: 0.943, acc: 0.689   Evaluation on val [42/66] loss: 0.981, acc: 0.682   Evaluation on val [43/66] loss: 0.982, acc: 0.680   Evaluation on val [44/66] loss: 0.965, acc: 0.685   Evaluation on val [45/66] loss: 0.953, acc: 0.686   Evaluation on val [46/66] loss: 0.951, acc: 0.688   Evaluation on val [47/66] loss: 0.953, acc: 0.686   Evaluation on val [48/66] loss: 0.954, acc: 0.685   Evaluation on val [49/66] loss: 0.937, acc: 0.691   Evaluation on val [50/66] loss: 0.944, acc: 0.688   Evaluation on val [51/66] loss: 0.938, acc: 0.689   Evaluation on val [52/66] loss: 0.923, acc: 0.692   Evaluation on val [53/66] loss: 0.912, acc: 0.696   Evaluation on val [54/66] loss: 0.905, acc: 0.699   Evaluation on val [55/66] loss: 0.916, acc: 0.700   Evaluation on val [56/66] loss: 0.915, acc: 0.701   Evaluation on val [57/66] loss: 0.911, acc: 0.697   Evaluation on val [58/66] loss: 0.897, acc: 0.703   Evaluation on val [59/66] loss: 0.900, acc: 0.703   Evaluation on val [60/66] loss: 0.890, acc: 0.706   Evaluation on val [61/66] loss: 0.880, acc: 0.707   Evaluation on val [62/66] loss: 0.875, acc: 0.706   Evaluation on val [63/66] loss: 0.879, acc: 0.704   Evaluation on val [64/66] loss: 0.869, acc: 0.707   Evaluation on val [65/66] loss: 0.870, acc: 0.708   Evaluation on val [66/66] loss: 0.875, acc: 0.706   Evaluation on val [66/66] loss: 0.875, acc: 0.706, eer: 0.073, f1_score: 0.697   
