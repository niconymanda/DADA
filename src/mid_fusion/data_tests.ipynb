{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import ASVSpoof21Dataset\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "\n",
    "SUPPORTED_FORMATS = [\"wav\", \"mp3\", \"flac\"]\n",
    "\n",
    "\n",
    "def load_audio(filename, sampling_rate=None):\n",
    "    # Load audio file\n",
    "    assert os.path.exists(filename), f\"File {filename} does not exist\"\n",
    "    assert (\n",
    "        filename.split(\".\")[-1] in SUPPORTED_FORMATS\n",
    "    ), f\"File {filename} is not supported\"\n",
    "\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(filename, sr=None)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def pad(x, max_len):\n",
    "    \"\"\"\n",
    "    From src/baselines/asvspoof2021/DF/Baseline-RawNet2/data_utils.py\n",
    "    \"\"\"\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def get_spoof_list(meta_dir, is_train=False, is_eval=False):\n",
    "    d_meta = {}\n",
    "    file_list = []\n",
    "    with open(meta_dir, \"r\") as f:\n",
    "        l_meta = f.readlines()\n",
    "\n",
    "    if is_train:\n",
    "        for line in l_meta:\n",
    "            # _, key, _, _, label = line.strip().split(\" \")\n",
    "\n",
    "            key, label = line.split(\" \")[1], line.split(\" \")[5]\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n",
    "\n",
    "    elif is_eval:\n",
    "        for line in l_meta:\n",
    "            key = line.strip()\n",
    "            file_list.append(key)\n",
    "        return None, file_list\n",
    "    else:\n",
    "        for line in l_meta:\n",
    "            key, label = line.split(\" \")[1], line.split(\" \")[5]\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n",
    "\n",
    "\n",
    "class ASVSpoof21Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        meta_dir,\n",
    "        is_train=False,\n",
    "        is_eval=False,\n",
    "        sampling_rate=16000,\n",
    "        max_duration=4,\n",
    "        get_transcription=False,\n",
    "    ):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_duration = max_duration\n",
    "        self.cut = self.sampling_rate * self.max_duration  # padding\n",
    "        self.meta, self.list_IDs = get_spoof_list(meta_dir, is_train, is_eval)\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def load_audio_tensor(self, key):\n",
    "        filename = os.path.join(self.root_dir, f\"flac/{key}.flac\")\n",
    "        audio_arr, _ = load_audio(filename, self.sampling_rate)\n",
    "        audio_tensor = torch.tensor(pad(audio_arr, self.cut)).float()\n",
    "        return audio_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f = self.list_IDs[idx]\n",
    "        y = self.meta[f]\n",
    "        x = self.load_audio_tensor(f)\n",
    "        meta = {\"key\": f, \"label\": y}\n",
    "        return x, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ASVSpoof21Dataset(\n",
    "    \"/data/amathur-23/DADA/ASVspoof2021_DF_eval\",\n",
    "    \"/data/amathur-23/DADA/ASVspoof2021_DF_eval/keys/DF/CM/trial_metadata.txt\",\n",
    "    is_train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(meta):\n",
    "    return [os.path.join(ds.root_dir, f\"flac/{key}.flac\") for key in meta[\"key\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "whisper = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    \"openai/whisper-large-v3\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda:2\",\n",
    ")\n",
    "\n",
    "transcription = whisper(get_files(y))\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "keys = []\n",
    "transcriptions = []\n",
    "\n",
    "for _, meta in tqdm(loader):\n",
    "    keys.extend(meta[\"key\"])\n",
    "    transcription_list_dict = whisper(get_files(meta))\n",
    "    transcriptions.extend([x[\"text\"] for x in transcription_list_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"key\": keys, \"transcription\": transcriptions})\n",
    "df.to_csv(\"asvspoof21_df_eval_transcriptions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
