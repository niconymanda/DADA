{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "wiki_data = pd.read_csv(\"/data/iivanova-23/data/wiki/wiki_quotes.csv\")\n",
    "# train, val = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=42, shuffle=True)\n",
    "wiki_data['label'].value_counts()\n",
    "unique_authors = wiki_data['label'].unique()\n",
    "print(len(unique_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "{34, 38, 7, 39, 10, 42, 43, 45, 14, 46, 17, 20, 52, 24, 25, 26, 31}\n"
     ]
    }
   ],
   "source": [
    "data_in_the_wild = pd.read_csv(\"/data/iivanova-23/data/inthewild/inthewild_transcriptions_final.csv\")\n",
    "data_in_the_wild['label'].value_counts()\n",
    "unique_authors_inthewild = data_in_the_wild['label'].unique()\n",
    "print(len(unique_authors_inthewild))\n",
    "print(set(unique_authors_inthewild) - set(unique_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [37, 28, 51, 0, 8, 49, 16, 34, 38, 7, 39, 10, 42, 43, 45, 14, 46, 17, 20, 52, 24, 25, 26, 31]\n",
    "data_wild_small = data_in_the_wild[data_in_the_wild['label'].isin(labels)]\n",
    "data_concat = pd.concat([wiki_data, data_wild_small])\n",
    "data_concat.to_csv(\"/data/iivanova-23/data/wild_wiki.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data_concat, test_size=0.3, stratify=data_concat['label'], random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alec Guinness': 0,\n",
       " 'Barack Obama': 1,\n",
       " 'Christopher Hitchens': 2,\n",
       " 'Ayn Rand': 3,\n",
       " 'Donald Trump': 4,\n",
       " 'Bernie Sanders': 5,\n",
       " 'Calvin Coolidge': 6,\n",
       " 'Winston Churchill': 7,\n",
       " 'Bob Ross': 8,\n",
       " 'Richard Nixon': 9,\n",
       " '2Pac': 10,\n",
       " 'Ronald Reagan': 11,\n",
       " 'John F. Kennedy': 12,\n",
       " 'Arnold Schwarzenegger': 13,\n",
       " 'Nick Offerman': 14,\n",
       " 'Bill Clinton': 15,\n",
       " 'Martin Luther King': 16,\n",
       " 'Bill Burr': 17,\n",
       " 'Mark Zuckerberg': 18,\n",
       " 'George W. Bush': 19,\n",
       " 'Dave Chappelle': 20,\n",
       " 'Boris Johnson': 21,\n",
       " 'Alan Watts': 22,\n",
       " 'Louis C.K.': 23,\n",
       " 'The Notorious B.I.G.': 24,\n",
       " 'Jimmy Carter': 25,\n",
       " 'Dwight Eisenhower': 26,\n",
       " 'Mitch Hedberg': 27,\n",
       " 'Alexandria Ocasio-Cortez': 28,\n",
       " 'Milton Friedman': 29,\n",
       " 'Franklin D. Roosevelt': 30,\n",
       " 'Queen Elizabeth II': 31,\n",
       " 'Jerry Seinfeld': 32,\n",
       " 'Tucker Carlson': 33,\n",
       " 'Nelson Mandela': 34,\n",
       " 'John Cleese': 35,\n",
       " 'Frank Sinatra': 36,\n",
       " 'Louis Farrakhan': 37,\n",
       " 'Norm MacDonald': 38,\n",
       " 'Gilbert Gottfried': 39,\n",
       " 'George Carlin': 40,\n",
       " 'Malcolm X': 41,\n",
       " 'Robert Kardashian': 42,\n",
       " 'Kamala Harris': 43,\n",
       " 'Orson Welles': 44,\n",
       " 'Adam Driver': 45,\n",
       " 'Lyndon Johnson': 46,\n",
       " 'Fred Rogers': 47,\n",
       " 'Billie Eilish': 48,\n",
       " 'Jeff Goldblum': 49,\n",
       " 'Harry S. Truman': 50,\n",
       " 'Scarlett Johansson': 51,\n",
       " 'Kanye West': 52,\n",
       " 'William F. Buckley': 53}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = pd.read_csv('/data/amathur-23/DADA/InTheWild/release_in_the_wild/meta.csv')\n",
    "meta_data['speaker'] = meta_data['speaker'].replace('JFK', 'John F. Kennedy').replace('FDR', 'Franklin D. Roosevelt').replace('Harry Truman', 'Harry S. Truman').replace('Mr. Rogers','Fred Rogers')\n",
    "author_id_map = {author: i for i, author in enumerate(meta_data['speaker'].unique())}\n",
    "author_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_id_map['Winston Churchill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN labels:\n",
      "            file         author_name       type  \\\n",
      "16        16.wav   Winston Churchill      spoof   \n",
      "17        17.wav   Winston Churchill      spoof   \n",
      "22        22.wav                2Pac  bona-fide   \n",
      "28        28.wav       Nick Offerman  bona-fide   \n",
      "31        31.wav           Bill Burr      spoof   \n",
      "...          ...                 ...        ...   \n",
      "31752  31755.wav   Winston Churchill      spoof   \n",
      "31753  31756.wav  Queen Elizabeth II      spoof   \n",
      "31762  31765.wav   Winston Churchill  bona-fide   \n",
      "31763  31766.wav   Robert Kardashian  bona-fide   \n",
      "31768  31771.wav   Winston Churchill      spoof   \n",
      "\n",
      "                                                    text  index  tokens  label  \n",
      "16           We all bleed the same regrowth of patriots.     17       8    NaN  \n",
      "17               and spent trillions of dollars overseas     18       6    NaN  \n",
      "22                 lending money to everybody except us.     23       6    NaN  \n",
      "28                                            All right.     29       2    NaN  \n",
      "31      and shining morning face creeping like snail ...     32      40    NaN  \n",
      "...                                                  ...    ...     ...    ...  \n",
      "31752               redoubling our trading relationship,  31753       4    NaN  \n",
      "31753                    And that's just with my Bahans.  31754       6    NaN  \n",
      "31762   increases the power of our long distance bloc...  31763       8    NaN  \n",
      "31763                    You were soul of my pro career.  31764       7    NaN  \n",
      "31768                 We've enriched the marine industry  31769       5    NaN  \n",
      "\n",
      "[3890 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "in_the_wild = pd.read_csv(\"/data/iivanova-23/data/inthewild/inthewild_transcriptions_final.csv\")\n",
    "# real = in_the_wild[in_the_wild['type'] == 'bona-fide']\n",
    "# len(real) \n",
    "# # Check for NaN values in the 'label' column\n",
    "# nan_labels = in_the_wild[in_the_wild['label'].isna()]\n",
    "# print(\"Rows with NaN labels:\")\n",
    "# print(nan_labels)\n",
    "# in_the_wild['label'] = in_the_wild['author_name'].map(author_id_map)\n",
    "\n",
    "# Proceed with train_test_split if there are no NaN values\n",
    "# if nan_labels.empty:\n",
    "#     test, train = train_test_split(real, test_size=0.3, random_state=42, stratify=real['label'])\n",
    "# else:\n",
    "#     print(\"Cannot perform train_test_split due to NaN values in 'label' column.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_the_wild['label'] = in_the_wild['author_name'].map(author_id_map)\n",
    "real = in_the_wild[in_the_wild['type'] == 'bona-fide']\n",
    "train, test = train_test_split(real, test_size=0.3, random_state=42, stratify=real['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(real['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_the_wild.to_csv('/data/iivanova-23/data/inthewild/inthewild_transcriptions_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_triplets(df):\n",
    "    triplets = []\n",
    "    for i, row in df.iterrows():\n",
    "        anchor = row\n",
    "        anchor_label = row['label']\n",
    "        positive_sample = df[df['label'] == anchor_label].sample(n=1).iloc[0]\n",
    "        negative_sample = df[df['label'] != anchor_label].sample(n=1).iloc[0]\n",
    "        triplets.append((anchor['index'], positive_sample['index'], negative_sample['index']))\n",
    "    triplets = [(int(a), int(p), int(n)) for a, p, n in triplets]\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxCeleb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/amathur-23/DADA/VoxCeleb2/'\n",
    "train = os.path.join(data, 'train')\n",
    "train_0 = os.listdir(train)[0]\n",
    "train_data = []\n",
    "with open(os.path.join(train, train_0)) as f:\n",
    "    file_data = json.load(f)\n",
    "    train_data.append([file_data['transcription'], file_data['speaker_id'], file_data['gender']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like I'm divorced, but now he wants  me to co...</td>\n",
       "      <td>id00649</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       transcription speaker_id  gender\n",
       "0   like I'm divorced, but now he wants  me to co...    id00649  female"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['transcription', 'speaker_id', 'gender'])  \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = '/data/iivanova-23/data/VoxCeleb2/val/meta.csv'\n",
    "data = pd.read_csv(data)\n",
    "data['file_path'] = data['file_path'].str.replace('/amathur-23/DADA/', '/iivanova-23/data/')\n",
    "data.to_csv('/data/iivanova-23/data/VoxCeleb2/val/meta.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/amathur-23/DADA/VoxCeleb2/train/meta.csv'\n",
    "meta_df = pd.read_csv(data)\n",
    "len(meta_df['speaker_id'].unique())\n",
    "samples_per_speaker = meta_df['speaker_id'].value_counts()\n",
    "top_100_speakers = samples_per_speaker.head(10).index\n",
    "meta_df_small = meta_df[meta_df['speaker_id'].isin(top_100_speakers)]\n",
    "unique_speakers = meta_df_small['speaker_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/amathur-23/DADA/VoxCeleb2/'\n",
    "train = os.path.join(data, 'train')\n",
    "train_data = []\n",
    "label_speaker_map = {}\n",
    "for file in os.listdir(train)[:5000]:\n",
    "    with open(os.path.join(train, file), 'r') as f: \n",
    "        file_data = json.load(f)\n",
    "        train_data.append([file_data['transcription'], file_data['speaker_id'], file_data['gender']])\n",
    "train_df = pd.DataFrame(train_data, columns=['transcription', 'speaker_id', 'gender'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASVspoof 2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51998\n",
      "1934\n",
      "count    4934.000000\n",
      "mean        8.572152\n",
      "std         6.037349\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%         8.000000\n",
      "75%        10.000000\n",
      "max       333.000000\n",
      "Name: tokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "csv_file = pd.read_csv('/home/infres/iivanova-23/DADA/iivanova-23/data/asvspoof2021/transcriptions_gpu.csv')\n",
    "print(len(csv_file[csv_file['type']=='spoof']))\n",
    "print(len(csv_file[csv_file['type']!='spoof']))\n",
    "spoof_small = csv_file[csv_file['type']=='spoof'].sample(3000)\n",
    "bonafide_small = csv_file[csv_file['type']!='spoof']\n",
    "data_asv = pd.concat([spoof_small, bonafide_small])\n",
    "data_asv['index'] = range(1, len(data_asv) + 1)\n",
    "data_asv['tokens'] = data_asv['transcription'].apply(lambda x: len(x.split()))\n",
    "print(data_asv['tokens'].describe())\n",
    "data_asv.to_csv('/home/infres/iivanova-23/DADA/iivanova-23/data/asvspoof2021/transcriptions_small.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In The Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_in_the_wild = pd.read_csv('/data/amathur-23/DADA/InTheWild/release_in_the_wild/meta.csv')\n",
    "unique_authors_meta = meta_in_the_wild['speaker'].unique()\n",
    "meta_in_the_wild['speaker'] = meta_in_the_wild['speaker'].replace('JFK', 'John F. Kennedy').replace('FDR', 'Franklin D. Roosevelt').replace('Harry Truman', 'Harry S. Truman').replace('Mr. Rogers','Fred Rogers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_wiki = pd.read_csv('~/DADA/Data/WikiQuotes_train.csv')\n",
    "# data['index'] = range(1, len(data) + 1)\n",
    "# data = data.to_csv('~/DADA/Data/WikiQuotes_train.csv', index=False)\n",
    "# data_wiki = data_wiki[data_wiki['type'] != 'spoof']\n",
    "# data_authors = data_wiki[data_wiki['label'] < 10]\n",
    "# data_wild = pd.read_csv('/data/iivanova-23/DataDADA/wild_transcription_meta.csv')\n",
    "# data_wild['tokens'] = data_wild['text'].apply(lambda x: len(x.split()))\n",
    "# data_wild = data_wild[(data_wild['tokens'] > 5) & (data_wild['tokens'] < 40)]\n",
    "# data_wild = data_wild[data_wild['label'] < 10]\n",
    "# data_merged = pd.concat([data_authors, data_wild])\n",
    "# # len(data_merged)\n",
    "# data_real = data_merged[data_merged['type'] != 'spoof']\n",
    "# data_spoof = data_merged[data_merged['type'] == 'spoof']\n",
    "# train, test = train_test_split(data_real, test_size=0.3, random_state=42)\n",
    "# data_merged.to_csv('/data/iivanova-23/DataDADA/train_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the wild dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_authors_create_dataFrame(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    unique_authors = set()\n",
    "    authors_list = []\n",
    "    ids_list = []\n",
    "    quotes_list = []\n",
    "    types_list = []\n",
    "    author_id_map = {}\n",
    "    current_id = 0\n",
    "    for item in data:\n",
    "        author = item.get('speaker')\n",
    "        if author:\n",
    "            if author not in author_id_map:\n",
    "                    author_id_map[author] = current_id\n",
    "                    current_id += 1\n",
    "            unique_authors.add(author)\n",
    "            authors_list.append(author)\n",
    "            ids_list.append(author_id_map[author])\n",
    "            quotes_list.append(item.get('content'))\n",
    "            types_list.append(item.get('label'))\n",
    "    data = pd.DataFrame({\n",
    "        'author_id': ids_list,\n",
    "        'author_name': authors_list,\n",
    "        'text': quotes_list,\n",
    "        'type': types_list,\n",
    "        'length': [len(quote) for quote in quotes_list]\n",
    "    })\n",
    "    data['author_name'] = data['author_name'].replace('JFK', 'John F. Kennedy').replace('FDR', 'Franklin D. Roosevelt').replace('Harry Truman', 'Harry S. Truman').replace('Mr. Rogers','Fred Rogers')\n",
    "    author_id_map['John F. Kennedy'] = author_id_map.pop('JFK')\n",
    "    author_id_map['Franklin D. Roosevelt'] = author_id_map.pop('FDR')\n",
    "    author_id_map['Harry S. Truman'] = author_id_map.pop('Harry Truman')\n",
    "    author_id_map['Fred Rogers'] = author_id_map.pop('Mr. Rogers')\n",
    "    return unique_authors, data, author_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_author_quotes_count(data):\n",
    "    quote_counts = data['author_name'].value_counts()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    quote_counts.plot(kind='bar', color=plt.cm.viridis(np.linspace(0, 1, len(quote_counts))))\n",
    "    plt.title('Number of Quotes per Author')\n",
    "    plt.xlabel('Author')\n",
    "    plt.ylabel('Number of Quotes')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    return quote_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikiquotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_count(data):\n",
    "    data['tokens'] = data['text'].apply(lambda x: len(x.split()))\n",
    "    return data\n",
    "\n",
    "def plot_tokens_distribution(data):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='author_name', y='tokens', data=data, palette='blend:#7AB,#EDA', hue='author_name')\n",
    "    plt.title('Tokens Distribution per Author')\n",
    "    plt.xlabel('Author')\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_data = pd.read_csv('/data/iivanova-23/data/wild_transcription_meta.csv')\n",
    "wild_data = wild_data.drop(columns=['author_id', 'index', 'label'])\n",
    "wild_data['tokens'] = wild_data['text'].apply(lambda x: len(x.split()))\n",
    "wild_data = wild_data[~wild_data['author_name'].isin(['Jimmy Carter', 'Dave Chappelle'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = wild_data['author_name'].unique()\n",
    "author_id_map = {author: i for i, author in enumerate(labels)}\n",
    "wild_data['label'] = wild_data['author_name'].map(author_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_quotes = pd.read_csv('/data/iivanova-23/data/wiki/wiki_quotes.csv')\n",
    "author_id_map = wiki_quotes[['author_name', 'label']].drop_duplicates().set_index('author_name').to_dict()['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_in_the_wild = pd.read_csv('/data/amathur-23/DADA/InTheWild/release_in_the_wild/meta.csv')\n",
    "meta_in_the_wild['speaker'] = meta_in_the_wild['speaker'].replace('JFK', 'John F. Kennedy').replace('FDR', 'Franklin D. Roosevelt').replace('Harry Truman', 'Harry S. Truman').replace('Mr. Rogers','Fred Rogers')\n",
    "unique_authors_meta = meta_in_the_wild['speaker'].unique()\n",
    "\n",
    "transciptions = pd.read_csv('/home/infres/iivanova-23/DADA/iivanova-23/data/inthewild/in_the_wild_transcriptions.csv')\n",
    "transciptions['speaker'] = transciptions['speaker'].replace('JFK', 'John F. Kennedy').replace('FDR', 'Franklin D. Roosevelt').replace('Harry Truman', 'Harry S. Truman').replace('Mr. Rogers','Fred Rogers')\n",
    "transciptions = transciptions.rename(columns={'label': 'type'})\n",
    "transciptions = transciptions.rename(columns={'speaker': 'author_name'})\n",
    "transciptions = transciptions.rename(columns={'transcription': 'text'})\n",
    "transciptions['label']  = transciptions['author_name'].map(author_id_map)\n",
    "transciptions.to_csv('/data/iivanova-23/data/inthewild/inthewild_transcriptions_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikiquotes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643076/3898041049.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_quotes_df['author_name'] = filtered_quotes_df['source_id'].map(source_id_to_name)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = '/data/iivanova-23/data/'\n",
    "metadata_file = os.path.join(folder_path, 'quotes-500-en-sources.csv')\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "quotes_file = os.path.join(folder_path, 'quotes-500-en-quotes.csv')\n",
    "quotes_df = pd.read_csv(quotes_file)\n",
    "filtered_quotes_df = quotes_df[quotes_df['source_id'].isin(metadata_df[metadata_df['name'].isin(author_id_map.keys())]['source_id'])]\n",
    "source_id_to_name = dict(zip(metadata_df['source_id'], metadata_df['name']))\n",
    "filtered_quotes_df['author_name'] = filtered_quotes_df['source_id'].map(source_id_to_name)\n",
    "wiki_data = pd.DataFrame({\n",
    "    'label': filtered_quotes_df['author_name'].map(author_id_map),\n",
    "    'author_name': filtered_quotes_df['author_name'],\n",
    "    'text': filtered_quotes_df['quote'],\n",
    "    'type': 'bona-fide',\n",
    "    'length': filtered_quotes_df['quote'].apply(len),\n",
    "    'tokens': filtered_quotes_df['quote'].apply(lambda x: len(x.split()))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344 1005\n"
     ]
    }
   ],
   "source": [
    "def truncate_data(data, max_tokens):\n",
    "    data_new = []\n",
    "    for i in range(len(data)):\n",
    "        text = data.iloc[i]['text']\n",
    "        words = text.split()\n",
    "        tokens = data.iloc[i]['tokens']\n",
    "        for j in range(0, tokens, max_tokens):\n",
    "            chunk = ' '.join(words[j:j + max_tokens])\n",
    "            # print(chunk, len(chunk.split()))\n",
    "            if len(chunk.split()) > 5:\n",
    "                data_new.append({\n",
    "                    'text': chunk,\n",
    "                    'author_name': data.iloc[i]['author_name'],\n",
    "                    'tokens': len(chunk.split()), \n",
    "                    'label': data.iloc[i]['label'],\n",
    "                    'length': len(chunk), \n",
    "                    'type': data.iloc[i]['type'],\n",
    "                    \n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data_new)\n",
    "    return df\n",
    "\n",
    "wiki_quotes_truncated = truncate_data(wiki_data, 64)\n",
    "wiki_quotes_truncated['index'] = range(1, len(wiki_quotes_truncated) + 1)\n",
    " \n",
    "wiki_quotes_truncated.to_csv('/data/iivanova-23/data/wiki/wiki_quotes.csv', index=False)\n",
    "train, test = train_test_split(wiki_quotes_truncated, test_size=0.3, random_state=42)\n",
    "train_triples = form_triplets(train)\n",
    "test_triples = form_triplets(test)\n",
    "# spoofed_triplets = form_triplets(all_data_spoof)\n",
    "print(len(train_triples), len(test_triples))\n",
    "\n",
    "data_json = {'train': train_triples, 'test': test_triples}\n",
    "\n",
    "with open('/data/iivanova-23/data/wiki/meta_wiki.json', 'w') as f:\n",
    "    json.dump(data_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dadaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
